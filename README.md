---
title: Ultra Tic-Tac-Toe AI
emoji: ðŸŽ®
colorFrom: blue
colorTo: purple
sdk: gradio
sdk_version: "6.5.0"
python_version: "3.12"
app_file: app.py
pinned: false
---

# Ultra Tic-Tac-Toe AI

**Pure AlphaZero Implementation for Ultimate Tic-Tac-Toe with DTW Endgame Solver**

A high-performance AI engine that combines deep reinforcement learning (AlphaZero-style) with perfect endgame play using Distance-To-Win (DTW) alpha-beta search.

## Features

- **AlphaZero-style Training**: Self-play reinforcement learning with MCTS
- **DTW Endgame Solver**: Perfect play in endgame positions (â‰¤15 empty cells)
- **High Performance**: Cython + C++ optimizations for critical paths
- **TensorRT Inference**: Dedicated TRT server process for fast self-play
- **Lc0-style Training**: Large-scale self-play with optimized replay buffer
- **Auto Setup**: Extensions auto-build on first run; checkpoints auto-sync with HuggingFace

---

## Project Structure

```
ultra-tictactoe/
â”œâ”€â”€ app.py                    # Gradio web interface
â”œâ”€â”€ setup.py                  # Cython build configuration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ packages.txt              # System dependencies (HF Spaces)
â”‚
â”œâ”€â”€ ai/                       # AI module
â”‚   â”œâ”€â”€ config.py             # Training configuration
â”‚   â”œâ”€â”€ train.py              # Training loop
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                 # Neural network
â”‚   â”‚   â”œâ”€â”€ network.py        # Model architecture (ResNet + SE)
â”‚   â”‚   â”œâ”€â”€ alpha_zero_net.py # Training wrapper (optimizer, AMP, torch.compile)
â”‚   â”‚   â””â”€â”€ tensorrt_engine.py # TensorRT inference server (separate process)
â”‚   â”‚
â”‚   â”œâ”€â”€ mcts/                 # Monte Carlo Tree Search
â”‚   â”‚   â”œâ”€â”€ node.py           # Python MCTS node
â”‚   â”‚   â”œâ”€â”€ node_cy.pyx       # Cython MCTS node (optimized)
â”‚   â”‚   â”œâ”€â”€ mcts.py           # MCTS algorithm
â”‚   â”‚   â””â”€â”€ agent.py          # AlphaZero agent
â”‚   â”‚
â”‚   â”œâ”€â”€ endgame/              # DTW Endgame Solver
â”‚   â”‚   â”œâ”€â”€ dtw_calculator.py # DTW interface
â”‚   â”‚   â””â”€â”€ transposition_table.py # Cache (hot/cold storage)
â”‚   â”‚
â”‚   â”œâ”€â”€ training/             # Training components
â”‚   â”‚   â”œâ”€â”€ self_play.py      # Self-play game generation
â”‚   â”‚   â”œâ”€â”€ replay_buffer.py  # Lc0-style replay buffer
â”‚   â”‚   â””â”€â”€ trainer.py        # Training orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ baselines/            # Baseline agents
â”‚   â”‚   â”œâ”€â”€ random_agent.py   # Random moves
â”‚   â”‚   â”œâ”€â”€ heuristic_agent.py # Rule-based
â”‚   â”‚   â””â”€â”€ minimax_agent.py  # Minimax search
â”‚   â”‚
â”‚   â””â”€â”€ prediction/           # Inference utilities
â”‚       â””â”€â”€ prediction_agent.py
â”‚
â”œâ”€â”€ game/                     # Game logic
â”‚   â”œâ”€â”€ board.py              # Python board implementation
â”‚   â”œâ”€â”€ board_cy.pyx          # Cython board (training)
â”‚   â””â”€â”€ __init__.py           # Board import selector
â”‚
â”œâ”€â”€ game/cpp/                 # C++ board (pybind11)
â”‚   â”œâ”€â”€ board.cpp             # C++ board implementation
â”‚   â””â”€â”€ board.hpp
â”‚
â”œâ”€â”€ ai/endgame/cpp/           # C++ DTW (pybind11)
â”‚   â”œâ”€â”€ dtw.cpp               # C++ DTW alpha-beta search
â”‚   â””â”€â”€ dtw.hpp
â”‚
â”œâ”€â”€ cpp_bindings.cpp          # pybind11 bindings
â”œâ”€â”€ setup_cpp.py              # C++ build configuration
â”‚
â”œâ”€â”€ utils/                    # Utilities
â”‚   â”œâ”€â”€ hf_upload.py          # HuggingFace upload/download
â”‚   â””â”€â”€ __init__.py
â”‚
â””â”€â”€ model/                    # Saved models & cache
    â”œâ”€â”€ runs.json             # Run registry (synced to HF)
    â”œâ”€â”€ <run_id>/latest.pt    # Latest checkpoint per run
    â”œâ”€â”€ <run_id>/best.pt      # Best checkpoint per run
    â”œâ”€â”€ dtw_cache.pkl         # DTW transposition table
    â””â”€â”€ <run_id>/training.log # Training logs
```

---

## Model Architecture

### Neural Network (AlphaZero-style ResNet)

```
Input: 7 channels Ã— 9 Ã— 9
â”œâ”€â”€ 7 input planes:
â”‚   â”œâ”€â”€ [0-1] Current player pieces (per sub-board)
â”‚   â”œâ”€â”€ [2-3] Opponent pieces (per sub-board)
â”‚   â”œâ”€â”€ [4]   Valid moves mask
â”‚   â”œâ”€â”€ [5]   Sub-board completion status
â”‚   â””â”€â”€ [6]   Current player indicator

Backbone: 20 Residual Blocks Ã— 256 channels
â”œâ”€â”€ Each block:
â”‚   â”œâ”€â”€ Conv2d 3Ã—3 â†’ BatchNorm â†’ ReLU
â”‚   â”œâ”€â”€ Conv2d 3Ã—3 â†’ BatchNorm
â”‚   â”œâ”€â”€ SE Block (Squeeze-and-Excitation, reduction=16)
â”‚   â””â”€â”€ Skip connection â†’ ReLU

Policy Head:
â”œâ”€â”€ Conv2d 1Ã—1 â†’ BatchNorm â†’ ReLU
â”œâ”€â”€ Flatten â†’ Linear(162, 81)
â””â”€â”€ Output: 81 logits (one per cell)

Value Head:
â”œâ”€â”€ Conv2d 1Ã—1 â†’ BatchNorm â†’ ReLU
â”œâ”€â”€ Flatten â†’ Linear(81, 64) â†’ ReLU
â”œâ”€â”€ Linear(64, 1) â†’ Sigmoid
â””â”€â”€ Output: Win probability [0, 1]
```

**Model Size**: ~74M parameters (~274 MB)

### Key Design Choices

| Component | Choice | Rationale |
|-----------|--------|-----------|
| Residual Blocks | 20 | Balance between depth and training speed |
| Channels | 256 | Balance between capacity and speed |
| SE Blocks | Yes | Channel attention improves feature selection |
| Value Output | Sigmoid [0,1] | 0=loss, 0.5=draw, 1=win |
| Activation | ReLU | Standard, fast |

---

## Optimization Strategy

### 1. Cython Extensions (`*.pyx`)

**`game/board_cy.pyx`** - Game board logic

- Bitboard representation for sub-board states
- Fast move validation and application
- ~10x faster than pure Python

**`ai/mcts/node_cy.pyx`** - MCTS node operations

- Efficient child node management
- Optimized UCB calculation
- ~5x faster tree operations

**Build**: `python setup.py build_ext --inplace`

### 2. C++ Extensions (pybind11)

**`game/cpp/board.cpp`** - C++ board implementation

- Used for DTW search (fastest path)
- Minimal Python overhead

**`ai/endgame/cpp/dtw.cpp`** - Alpha-beta search

- Perfect endgame evaluation
- Transposition table integration
- ~20x faster than Python minimax

**Build**: `python setup_cpp.py build_ext --inplace`

### 3. TensorRT Inference

Self-play inference runs in a **dedicated TensorRT server process** to avoid CUDA context conflicts with PyTorch training:

- ONNX export â†’ TensorRT engine build (FP16)
- Zero-copy shared memory communication
- Automatic engine rebuild after each training iteration
- Fallback to `torch.compile` if TRT is unavailable

Disable TRT via environment variable if needed:
```bash
ULTRA_TRT_DISABLE=1 python -m ai.train
```

### 4. PyTorch Optimizations

| Optimization | Description |
|--------------|-------------|
| `torch.compile` | Graph compilation (fallback when TRT unavailable) |
| AMP (FP16) | Mixed precision training |
| Batch Inference | Parallel position evaluation |

### 5. Performance Summary

| Component | Implementation | Speedup |
|-----------|---------------|--------|
| Board Logic | Cython (bitboard) | ~10x |
| MCTS Node | Cython | ~5x |
| DTW Search | C++ | ~20x |
| Inference | TensorRT FP16 | ~3-5x vs eager |
| Network Training | AMP + torch.compile | ~2x |

---

## Training Methodology

### AlphaZero Algorithm

For each iteration:
    1. Self-Play: Generate 8,192 games using MCTS + current network
    2. Training: Update network on replay buffer samples
    3. Evaluation: (implicit via self-play improvement)

### Training Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| Iterations | 500 | Total training cycles |
| Games/Iteration | 8,192 | Self-play games per cycle |
| MCTS Simulations | 200 | Simulations per move |
| Batch Size | 2,048 | Training batch size |
| Training Epochs | 40 | Epochs per iteration |
| Learning Rate | 0.002 | Initial LR (cosine decay) |
| Weight Decay | 1e-4 | L2 regularization |
| Replay Buffer | 2M | Maximum samples |
| Parallel Games | 2,048 | Concurrent self-play games |

### Temperature Schedule

| Move Number | Temperature | Behavior |
|-------------|-------------|----------|
| 1-8 | 1.0 | Exploratory (proportional to visits) |
| 9+ | 0.0 | Greedy (best move only) |

### Loss Function

```
L = L_policy + L_value

L_policy = CrossEntropy(Ï€_predicted, Ï€_mcts)
L_value  = MSE(v_predicted, z_game_result)
```

---

## Data Collection (Self-Play)

### Parallel Self-Play

```python
# 256 games run in parallel
for batch in parallel_games:
    # 1. Get valid positions needing evaluation
    positions = [game.board for game in active_games]
    
    # 2. Batch neural network inference
    policies, values = network.predict_batch(positions)
    
    # 3. Run MCTS with network guidance
    for game, policy, value in zip(games, policies, values):
        mcts.search(game.board, prior_policy=policy)
        
    # 4. Select moves and record training data
    for game in games:
        move = select_move(mcts_policy, temperature)
        record_sample(board, mcts_policy, player)
```

### DTW Integration (Endgame)

When â‰¤15 empty cells remain:

1. **DTW Calculator** computes exact game-theoretic value
2. If **decisive** (win/loss for either player):
   - Skip remaining MCTS
   - Record position with perfect value
   - Early terminate game
3. Otherwise: Continue with MCTS

### Lc0-style Replay Buffer

| Feature | Description |
|---------|-------------|
| Game ID Tracking | Each sample tagged with game ID |
| Age-based Weighting | Recent games weighted higher |
| One Position Per Game | Batch sampling picks max 1 position per game |
| Deduplication | Reduces correlation in training batches |

```python
# Sampling strategy
weight = 1.0 / (1 + age_in_iterations * 0.1)
batch = sample_one_per_game(buffer, batch_size, weights)
```

---

## DTW Endgame Solver

### Distance-To-Win (DTW)

DTW measures how many moves until a forced win/loss:

- **DTW = +N**: Current player wins in N moves
- **DTW = -N**: Current player loses in N moves  
- **DTW = 0**: Draw with perfect play

### Alpha-Beta Search (C++)

```cpp
int alpha_beta(Board& board, int alpha, int beta, int depth) {
    // Transposition table lookup
    if (auto entry = tt.lookup(board.hash())) {
        return entry->value;
    }
    
    // Terminal check
    if (board.is_terminal()) {
        return evaluate_terminal(board);
    }
    
    // Search all moves
    int best = -INF;
    for (int move : board.valid_moves()) {
        board.make_move(move);
        int score = -alpha_beta(board, -beta, -alpha, depth + 1);
        board.undo_move(move);
        
        best = max(best, score);
        alpha = max(alpha, score);
        if (alpha >= beta) break;  // Pruning
    }
    
    tt.store(board.hash(), best);
    return best;
}
```

### Transposition Table

| Tier | Size | Purpose |
|------|------|---------|
| Hot Cache | 60M entries | Frequently accessed positions |
| Cold Cache | 240M entries | Archive for less common positions |

Cache is persisted to `dtw_cache.pkl` and uploaded to HuggingFace.

---

## Running the Project

### Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run Gradio app (extensions auto-build on startup)
python app.py
```

### Manual Extension Build (optional)

Extensions are auto-built when missing, but you can build manually:

```bash
# Cython extensions (board, MCTS node, board encoder)
python setup.py build_ext --inplace

# C++ extensions (board, DTW, NNUE)
python setup_cpp.py build_ext --inplace
```

### Training

```bash
# Start training (auto-downloads checkpoints from HF, interactive run selection)
python -m ai.train
```

On first run:
1. Missing `.so` extensions are **auto-built**
2. `runs.json` + checkpoints are **auto-downloaded** from HuggingFace
3. Select an existing run to resume or start a new one
4. Every iteration, `latest.pt` + `runs.json` are **auto-uploaded** to HuggingFace

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `HF_REPO_ID` | `sean2474/ultra-tictactoe-models` | HuggingFace model repo |
| `HF_UPLOAD` | `true` | Enable/disable HF uploads |
| `ULTRA_TRT_DISABLE` | unset | Set to `1` to disable TensorRT |

### HuggingFace Spaces

The app automatically builds Cython/C++ extensions on startup via `packages.txt` (g++) and build logic in `app.py`.

---

## Evaluation

### Baseline Tests

| Opponent | Expected Win Rate | Notes |
|----------|-------------------|-------|
| Random | >99% | Sanity check |
| Heuristic | >95% | Rule-based agent |
| Minimax-2 | >90% | 2-ply search |
| Minimax-3 | >80% | 3-ply search |

### Self-Play Metrics

- **Loss convergence**: ~1.8 after 30 iterations
- **DTW cache hit rate**: Increases over training
- **Average game length**: ~50 moves

---

## References

- [AlphaZero Paper](https://arxiv.org/abs/1712.01815) - Mastering Chess and Shogi
- [Lc0](https://lczero.org/) - Leela Chess Zero
- [Ultimate Tic-Tac-Toe Rules](https://en.wikipedia.org/wiki/Ultimate_tic-tac-toe)

---

## License

MIT License
